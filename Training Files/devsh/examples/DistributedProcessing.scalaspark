// Upload to HDFS
// $ hdfs dfs -put $DEVDATA/frostroad.txt
// $ hdfs dfs -put $DEVSH/examples/example-data/people.csv
// $ hdfs dfs -put $DEVSH/examples/example-data/people-no-header.csv
// $ hdfs dfs -put $DEVSH/examples/example-data/pcodes-no-header.csv
// $ hdfs dfs -put $DEVSH/examples/example-data/pcodes.csv


// Example: For each letter, calculate the average length of the words starting with that letter

val myfile = "frostroad.txt"

// specify 4 partitions to simulate a 4-block file
val avglens = sc.textFile(myfile,4).flatMap(line => line.split("\\W")).filter(words => words.length > 0).
  map(word => (word(0),word.length)).groupByKey(2).map(pair => (pair._1, pair._2.sum/pair._2.size.toDouble))

// call save to trigger the operations
avglens.saveAsTextFile("avglen-output")

// Catalyst execution plan
val peopleDF = spark.read.option("header","true").csv("people.csv")
val pcodesDF = spark.read.option("header","true").csv("pcodes.csv")
val joinedDF = peopleDF.join(pcodesDF, "pcode")
joinedDF.explain(true)

// RDD Execution plan
val peopleRDD = sc.textFile("people-no-header.csv").keyBy(s => s.split(',')(0))
val pcodesRDD = sc.textFile("pcodes-no-header.csv").keyBy(s => s.split(',')(0))
val joinedRDD = peopleRDD.join(pcodesRDD)
joinedRDD.toDebugString
