# Upload  to HDFS
# $ hdfs dfs -put $DEVSH/examples/example-data/purplecow.txt
# $ hdfs dfs -put $DEVSH/examples/example-data/userFiles/
# $ hdfs dfs -put $DEVSH/examples/example-data/cities.csv

# Create a text RDD
myRDD = spark.sparkContext.textFile("purplecow.txt")

# Create a whole file text RDD
userRDD = spark.sparkContext.wholeTextFiles("userFiles")

# Create an RDD from in-memory data
myData = ["Alice","Carlos","Frank","Barbara"]
myRDD = sc.parallelize(myData)

# Save an RDD as files
myRDD.saveAsTextFile("mydata/")

# Demo action operation
myRDD.count()
for line in myRDD.take(2): print line

# Demo distinct operation
distinctRDD = sc.textFile("cities1.csv").distinct()
for city in distinctRDD.collect(): print city

# Demo union operation
unionRDD = sc.textFile("cities2.csv").union(distinctRDD)
for city in unionRDD.collect(): print city
